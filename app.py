"""
LOF åŸºé‡‘å¥—åˆ©ç›‘æ§ç³»ç»Ÿ
ä½œè€…: è´¢åŠ¡å¥—åˆ©ä¸“å®¶
åŠŸèƒ½: ç›‘æ§ LOF åŸºé‡‘çš„åœºå¤–ç”³è´­ã€åœºå†…å–å‡ºå¥—åˆ©æœºä¼š
"""

import streamlit as st
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import warnings
import logging
import json
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥ akshare
try:
    import akshare as ak
    AKSHARE_AVAILABLE = True
    logger.info("âœ… Akshare æ¨¡å—åŠ è½½æˆåŠŸ")
except ImportError:
    AKSHARE_AVAILABLE = False
    logger.error("âŒ Akshare æœªå®‰è£…")

# ç¼“å­˜é…ç½®
CACHE_DIR = os.path.join(os.getcwd(), "lof_cache")
if not os.path.exists(CACHE_DIR):
    os.makedirs(CACHE_DIR, exist_ok=True)
    logger.info(f"ğŸ“ åˆ›å»ºç¼“å­˜ç›®å½•: {CACHE_DIR}")


def load_nav_cache(cache_date):
    """åŠ è½½æŒ‡å®šæ—¥æœŸçš„å‡€å€¼ç¼“å­˜"""
    cache_file = os.path.join(CACHE_DIR, f"nav_cache_{cache_date}.json")
    if os.path.exists(cache_file):
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
            logger.info(f"âœ… åŠ è½½ç¼“å­˜æ–‡ä»¶: {cache_file}ï¼Œå…± {len(cache_data)} æ¡æ•°æ®")
            return cache_data
        except Exception as e:
            logger.warning(f"âš ï¸ ç¼“å­˜æ–‡ä»¶è¯»å–å¤±è´¥: {str(e)}")
            return {}
    return {}


def save_nav_cache(cache_date, nav_dict):
    """ä¿å­˜å‡€å€¼ç¼“å­˜åˆ°æ–‡ä»¶"""
    cache_file = os.path.join(CACHE_DIR, f"nav_cache_{cache_date}.json")
    try:
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½å¯ä»¥è¢«JSONåºåˆ—åŒ–ï¼ˆè½¬æ¢æ—¥æœŸä¸ºå­—ç¬¦ä¸²ï¼‰
        serializable_dict = {}
        for code, data in nav_dict.items():
            serializable_dict[code] = {
                'åŸºé‡‘ä»£ç ': str(data['åŸºé‡‘ä»£ç ']),
                'åŸºé‡‘å‡€å€¼': float(data['åŸºé‡‘å‡€å€¼']),
                'å‡€å€¼æ—¥æœŸ': str(data['å‡€å€¼æ—¥æœŸ'])  # ç¡®ä¿æ—¥æœŸæ˜¯å­—ç¬¦ä¸²
            }
        
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(serializable_dict, f, ensure_ascii=False, indent=2)
        logger.info(f"âœ… ç¼“å­˜å·²ä¿å­˜: {cache_file}ï¼Œå…± {len(serializable_dict)} æ¡æ•°æ®")
    except Exception as e:
        logger.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {str(e)}", exc_info=True)


def fetch_single_nav(fund_code, start_date, end_date):
    """æŸ¥è¯¢å•åªåŸºé‡‘çš„å‡€å€¼ï¼ˆç”¨äºå¤šçº¿ç¨‹ï¼‰"""
    try:
        df_nav = ak.fund_etf_fund_info_em(
            fund=fund_code,
            start_date=start_date,
            end_date=end_date
        )
        
        if df_nav is not None and len(df_nav) > 0:
            latest_nav = df_nav.iloc[-1]
            return {
                'åŸºé‡‘ä»£ç ': fund_code,
                'åŸºé‡‘å‡€å€¼': latest_nav['å•ä½å‡€å€¼'],
                'å‡€å€¼æ—¥æœŸ': latest_nav['å‡€å€¼æ—¥æœŸ'],
                'success': True
            }
        else:
            return {'åŸºé‡‘ä»£ç ': fund_code, 'success': False, 'error': 'æ— å‡€å€¼æ•°æ®'}
    except Exception as e:
        return {'åŸºé‡‘ä»£ç ': fund_code, 'success': False, 'error': str(e)}


def get_lof_data():
    """è·å– LOF åŸºé‡‘å®æ—¶æ•°æ®"""
    if not AKSHARE_AVAILABLE:
        logger.error("âŒ Akshare æ¨¡å—æœªå®‰è£…ï¼Œæ— æ³•è·å–æ•°æ®")
        st.error("âŒ Akshare æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ï¼š`pip install akshare`")
        return None
    
    try:
        # ========== æ­¥éª¤ 1ï¼šè·å–LOFåœºå†…è¡Œæƒ…åˆ—è¡¨ ==========
        logger.info("ğŸ” [æ­¥éª¤1/3] å¼€å§‹è°ƒç”¨ Akshare API: fund_lof_spot_em() - è·å– LOF åœºå†…è¡Œæƒ…")
        
        df_market = ak.fund_lof_spot_em()
        
        logger.info(f"ğŸ“Š åœºå†…è¡Œæƒ…æ•°æ®è¡Œæ•°: {len(df_market)}")
        logger.info(f"ğŸ“‹ åœºå†…è¡Œæƒ…åˆ—å: {df_market.columns.tolist()}")
        logger.info(f"\nğŸ“„ å‰ 3 æ¡åŸå§‹æ•°æ®:\n{df_market.head(3).to_string()}")
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        required_columns = ['ä»£ç ', 'åç§°', 'æœ€æ–°ä»·', 'æˆäº¤é¢']
        missing_columns = [col for col in required_columns if col not in df_market.columns]
        
        if missing_columns:
            error_msg = f"åœºå†…è¡Œæƒ…æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_columns}"
            logger.error(f"âŒ {error_msg}")
            st.error(f"âŒ {error_msg}")
            return None
        
        # é‡å‘½ååˆ—
        df_market = df_market.rename(columns={
            'ä»£ç ': 'åŸºé‡‘ä»£ç ',
            'åç§°': 'åŸºé‡‘åç§°',
            'æœ€æ–°ä»·': 'åœºå†…ä»·æ ¼',
            'æˆäº¤é¢': 'åœºå†…æˆäº¤é¢'
        })
        
        # æ•°æ®ç±»å‹è½¬æ¢
        df_market['åœºå†…ä»·æ ¼'] = pd.to_numeric(df_market['åœºå†…ä»·æ ¼'], errors='coerce')
        df_market['åœºå†…æˆäº¤é¢'] = pd.to_numeric(df_market['åœºå†…æˆäº¤é¢'], errors='coerce')
        
        # åªä¿ç•™éœ€è¦çš„åˆ—
        df_market = df_market[['åŸºé‡‘ä»£ç ', 'åŸºé‡‘åç§°', 'åœºå†…ä»·æ ¼', 'åœºå†…æˆäº¤é¢']]
        logger.info(f"âœ… åœºå†…è¡Œæƒ…å¤„ç†å®Œæˆï¼Œå…± {len(df_market)} åª LOF")
        
        
        # ========== æ­¥éª¤ 2ï¼šä»ç¼“å­˜æˆ–APIè·å–å‡€å€¼æ•°æ® ==========
        cache_date = datetime.now().strftime("%Y%m%d")
        logger.info(f"ğŸ” [æ­¥éª¤2/3] æ£€æŸ¥ç¼“å­˜: {cache_date}")
        
        # åŠ è½½ç¼“å­˜
        nav_cache = load_nav_cache(cache_date)
        
        # ç¡®å®šå“ªäº›åŸºé‡‘éœ€è¦æŸ¥è¯¢
        fund_codes = df_market['åŸºé‡‘ä»£ç '].tolist()
        cached_codes = set(nav_cache.keys())
        need_fetch_codes = [code for code in fund_codes if code not in cached_codes]
        
        logger.info(f"ğŸ“¦ ç¼“å­˜å‘½ä¸­: {len(cached_codes)} åªï¼Œéœ€è¦æŸ¥è¯¢: {len(need_fetch_codes)} åª")
        
        nav_data = []
        
        # ä»ç¼“å­˜åŠ è½½å·²æœ‰æ•°æ®
        for code in fund_codes:
            if code in nav_cache:
                nav_data.append(nav_cache[code])
        
        # å¦‚æœæœ‰éœ€è¦æŸ¥è¯¢çš„åŸºé‡‘ï¼Œä½¿ç”¨å¤šçº¿ç¨‹æŸ¥è¯¢
        if need_fetch_codes:
            st.info(f"ğŸ”„ éœ€è¦æŸ¥è¯¢ {len(need_fetch_codes)} åªåŸºé‡‘çš„å‡€å€¼ï¼Œä½¿ç”¨3çº¿ç¨‹åŠ é€Ÿ...")
            logger.info(f"ğŸš€ å¼€å§‹å¤šçº¿ç¨‹æŸ¥è¯¢ï¼ˆ3çº¿ç¨‹ï¼‰...")
            
            end_date = datetime.now().strftime("%Y%m%d")
            start_date = (datetime.now() - timedelta(days=7)).strftime("%Y%m%d")
            
            success_count = 0
            fail_count = 0
            progress_bar = st.progress(0, text="æ­£åœ¨è·å–åŸºé‡‘å‡€å€¼...")
            
            # ä½¿ç”¨çº¿ç¨‹æ± ï¼Œ3ä¸ªçº¿ç¨‹å¹¶å‘
            with ThreadPoolExecutor(max_workers=3) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_code = {
                    executor.submit(fetch_single_nav, code, start_date, end_date): code
                    for code in need_fetch_codes
                }
                
                # æ”¶é›†ç»“æœ
                completed = 0
                for future in as_completed(future_to_code):
                    result = future.result()
                    completed += 1
                    
                    if result['success']:
                        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
                        nav_info = {
                            'åŸºé‡‘ä»£ç ': result['åŸºé‡‘ä»£ç '],
                            'åŸºé‡‘å‡€å€¼': result['åŸºé‡‘å‡€å€¼'],
                            'å‡€å€¼æ—¥æœŸ': result['å‡€å€¼æ—¥æœŸ']
                        }
                        nav_data.append(nav_info)
                        # æ›´æ–°ç¼“å­˜å­—å…¸
                        nav_cache[result['åŸºé‡‘ä»£ç ']] = nav_info
                        success_count += 1
                    else:
                        logger.warning(f"âš ï¸ {result['åŸºé‡‘ä»£ç ']} æŸ¥è¯¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                        fail_count += 1
                    
                    # æ›´æ–°è¿›åº¦æ¡
                    progress = completed / len(need_fetch_codes)
                    progress_bar.progress(progress, text=f"æ­£åœ¨è·å–åŸºé‡‘å‡€å€¼... ({completed}/{len(need_fetch_codes)})")
            
            progress_bar.empty()
            logger.info(f"âœ… æ–°æŸ¥è¯¢å®Œæˆï¼šæˆåŠŸ {success_count} åªï¼Œå¤±è´¥ {fail_count} åª")
            
            # ä¿å­˜æ›´æ–°åçš„ç¼“å­˜
            if success_count > 0:
                save_nav_cache(cache_date, nav_cache)
        else:
            st.success("âœ… å…¨éƒ¨æ•°æ®æ¥è‡ªç¼“å­˜ï¼Œæ— éœ€æŸ¥è¯¢API")
            logger.info("âœ… å…¨éƒ¨æ•°æ®æ¥è‡ªç¼“å­˜")
        
        if len(nav_data) == 0:
            st.error("âŒ æ— æ³•è·å–ä»»ä½•åŸºé‡‘çš„å‡€å€¼æ•°æ®")
            return None
        
        # è½¬æ¢ä¸º DataFrame
        df_nav = pd.DataFrame(nav_data)
        df_nav['åŸºé‡‘å‡€å€¼'] = pd.to_numeric(df_nav['åŸºé‡‘å‡€å€¼'], errors='coerce')
        
        logger.info(f"ğŸ“Š å‡€å€¼æ•°æ®æ€»æ•°: {len(df_nav)} æ¡")
        logger.info(f"\nğŸ“Š å‡€å€¼æ•°æ®å‰ 5 æ¡:\n{df_nav.head().to_string()}")
        
        
        # ========== æ­¥éª¤ 3ï¼šåˆå¹¶åœºå†…è¡Œæƒ…å’Œå‡€å€¼æ•°æ® ==========
        logger.info("ğŸ”— [æ­¥éª¤3/3] åˆå¹¶åœºå†…è¡Œæƒ…å’Œå‡€å€¼æ•°æ®")
        
        df = pd.merge(df_market, df_nav, on='åŸºé‡‘ä»£ç ', how='inner')  # å†…è¿æ¥ï¼Œåªä¿ç•™æœ‰å‡€å€¼çš„
        
        logger.info(f"ğŸ“Š åˆå¹¶åæ•°æ®è¡Œæ•°: {len(df)}")
        logger.info(f"\nğŸ“„ åˆå¹¶åå‰ 5 æ¡:\n{df.head().to_string()}")
        
        # æ·»åŠ è¾…åŠ©å­—æ®µ
        df['å®æ—¶ä¼°å€¼'] = df['åŸºé‡‘å‡€å€¼']
        
        # æ•°æ®æ¸…æ´—
        before_clean = len(df)
        df = df.dropna(subset=['åœºå†…ä»·æ ¼', 'åŸºé‡‘å‡€å€¼', 'åœºå†…æˆäº¤é¢'])
        df = df[df['åœºå†…ä»·æ ¼'] > 0]
        df = df[df['åŸºé‡‘å‡€å€¼'] > 0]
        after_clean = len(df)
        
        if before_clean > after_clean:
            logger.warning(f"âš ï¸ æ¸…ç†æ— æ•ˆæ•°æ®: {before_clean - after_clean} æ¡")
        
        result_df = df[['åŸºé‡‘ä»£ç ', 'åŸºé‡‘åç§°', 'åœºå†…ä»·æ ¼', 'åŸºé‡‘å‡€å€¼', 'å®æ—¶ä¼°å€¼', 'åœºå†…æˆäº¤é¢']]
        
        logger.info(f"âœ… æ•°æ®å¤„ç†å®Œæˆï¼Œæœ€ç»ˆè¿”å› {len(result_df)} æ¡æœ‰æ•ˆæ•°æ®")
        logger.info(f"\nğŸ“Š æœ€ç»ˆæ•°æ®å‰ 5 æ¡:\n{result_df.head().to_string()}")
        
        st.success(f"âœ… æˆåŠŸè·å– {len(result_df)} åª LOF åŸºé‡‘æ•°æ®ï¼ˆçœŸå®å‡€å€¼ï¼‰")
        
        return result_df
        
    except Exception as e:
        error_msg = f"è·å–æ•°æ®å¤±è´¥: {str(e)}"
        logger.error(f"âŒ {error_msg}", exc_info=True)
        st.error(f"âŒ {error_msg}")
        st.error(f"å¼‚å¸¸ç±»å‹: {type(e).__name__}")
        return None


def calculate_premium_rate(df):
    """è®¡ç®—æº¢ä»·ç‡"""
    df['æº¢ä»·ç‡(%)'] = ((df['åœºå†…ä»·æ ¼'] - df['åŸºé‡‘å‡€å€¼']) / df['åŸºé‡‘å‡€å€¼'] * 100).round(2)
    return df


def filter_opportunities(df, min_premium, min_turnover):
    """ç­›é€‰å¥—åˆ©æœºä¼š"""
    # è¿‡æ»¤æ¡ä»¶ï¼ˆç§»é™¤ç”³è´­çŠ¶æ€æ¡ä»¶ï¼Œå› ä¸ºæ˜¯æ¨¡æ‹Ÿæ•°æ®ï¼‰
    filtered = df[
        (df['æº¢ä»·ç‡(%)'] > min_premium) &
        (df['åœºå†…æˆäº¤é¢'] > min_turnover)
    ].copy()
    
    return filtered


def highlight_premium_level(row):
    """æ ¹æ®æº¢ä»·ç‡é«˜äº®æ˜¾ç¤º"""
    premium = row['æº¢ä»·ç‡(%)']
    
    if premium >= 5.0:
        # é«˜æº¢ä»·ï¼šçº¢è‰²é«˜äº®ï¼ˆé¸¡è…¿æœºä¼šï¼‰
        return ['background-color: #ffcccc; font-weight: bold; color: #d32f2f'] * len(row)
    elif premium >= 2.0:
        # ä¸­ç­‰æº¢ä»·ï¼šé»„è‰²é«˜äº®
        return ['background-color: #fff9c4; font-weight: bold; color: #f57c00'] * len(row)
    else:
        return [''] * len(row)


def format_turnover(value):
    """æ ¼å¼åŒ–æˆäº¤é¢æ˜¾ç¤º"""
    if value >= 10000:
        return f"{value/10000:.2f} ä¸‡"
    else:
        return f"{value:.2f} ä¸‡"


def main():
    """ä¸»ç¨‹åº"""
    # é¡µé¢é…ç½®
    st.set_page_config(
        page_title="LOF åŸºé‡‘å¥—åˆ©ç›‘æ§ç³»ç»Ÿ",
        page_icon="ğŸ’°",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # æ ‡é¢˜
    st.title("ğŸ’° LOF åŸºé‡‘å¥—åˆ©ç›‘æ§ç³»ç»Ÿ")
    st.markdown("### åœºå¤–ç”³è´­ã€åœºå†…å–å‡ºå¥—åˆ©æœºä¼šå®æ—¶ç›‘æ§")
    st.markdown("---")
    
    # ä¾§è¾¹æ å‚æ•°è®¾ç½®
    st.sidebar.header("ğŸ“Š ç­›é€‰å‚æ•°è®¾ç½®")
    
    min_premium = st.sidebar.slider(
        "æœ€å°æº¢ä»·ç‡ (%)",
        min_value=0.0,
        max_value=10.0,
        value=1.5,
        step=0.1,
        help="åªæ˜¾ç¤ºæº¢ä»·ç‡å¤§äºæ­¤å€¼çš„åŸºé‡‘"
    )
    
    min_turnover = st.sidebar.slider(
        "æœ€å°æˆäº¤é¢ (ä¸‡å…ƒ)",
        min_value=0,
        max_value=500,
        value=50,
        step=10,
        help="è¿‡æ»¤æµåŠ¨æ€§è¾ƒå·®çš„å“ç§"
    )
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("### ğŸ’¡ ä½¿ç”¨è¯´æ˜")
    st.sidebar.markdown("âš ï¸ **æ³¨æ„**ï¼šç”±äºæ— æ³•è·å–çœŸå®çš„ç”³è´­çŠ¶æ€å’Œé™é¢ï¼Œæ‰€ä»¥ç§»é™¤äº†è¿™äº›å­—æ®µã€‚ğŸ— é¸¡è…¿æœºä¼šåªæ ¹æ®æº¢ä»·ç‡åˆ¤æ–­ã€‚")
    
    # åˆ·æ–°æŒ‰é’®
    col1, col2 = st.columns([1, 5])
    with col1:
        if st.button("ğŸ”„ åˆ·æ–°æ•°æ®", width="stretch"):
            st.rerun()
    
    # è·å–æ•°æ®
    with st.spinner("æ­£åœ¨è·å– LOF åŸºé‡‘æ•°æ®..."):
        df = get_lof_data()
    
    if df is None or len(df) == 0:
        st.error("âŒ æ— æ³•è·å–æ•°æ®ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•")
        return
    
    # è®¡ç®—æº¢ä»·ç‡
    df = calculate_premium_rate(df)
    
    # ç­›é€‰æœºä¼š
    filtered_df = filter_opportunities(df, min_premium, min_turnover)
    
    # æŒ‰æº¢ä»·ç‡é™åºæ’åº
    filtered_df = filtered_df.sort_values('æº¢ä»·ç‡(%)', ascending=False)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    st.markdown("### ğŸ“ˆ æ•°æ®æ¦‚è§ˆ")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ€»LOFæ•°é‡", len(df))
    
    with col2:
        st.metric("ç¬¦åˆæ¡ä»¶", len(filtered_df))
    
    with col3:
        # ç»Ÿè®¡é¸¡è…¿æœºä¼šï¼ˆæº¢ä»·ç‡ >= 5%ï¼‰
        chicken_leg_count = len(filtered_df[filtered_df['æº¢ä»·ç‡(%)'] >= 5.0])
        st.metric("ğŸ— é¸¡è…¿æœºä¼š", chicken_leg_count, delta="æº¢ä»·â‰¥5%")
    
    with col4:
        if len(filtered_df) > 0:
            max_premium = filtered_df['æº¢ä»·ç‡(%)'].max()
            st.metric("æœ€é«˜æº¢ä»·ç‡", f"{max_premium:.2f}%")
        else:
            st.metric("æœ€é«˜æº¢ä»·ç‡", "N/A")
    
    st.markdown("---")
    
    # ä½¿ç”¨ Tab åˆ†åˆ«æ˜¾ç¤ºç­›é€‰ç»“æœå’Œå…¨é‡æ•°æ®
    tab1, tab2 = st.tabs(["ğŸ“‹ å¥—åˆ©æœºä¼šåˆ—è¡¨", "ğŸ“Š å…¨é‡LOFæ•°æ®"])
    
    with tab1:
        # æ˜¾ç¤ºç­›é€‰åçš„æ•°æ®è¡¨æ ¼
        if len(filtered_df) > 0:
            st.markdown("ğŸŸ¥ **çº¢è‰²** = é«˜æº¢ä»·(â‰¥5%) | ğŸŸ¡ **é»„è‰²** = ä¸­ç­‰æº¢ä»·(2-5%)")
            
            # å¯¹æ•°æ®åº”ç”¨æº¢ä»·ç‡åˆ†çº§é«˜äº®
            styled_df = filtered_df.style.apply(highlight_premium_level, axis=1)
            
            # æ ¼å¼åŒ–ç‰¹å®šåˆ—çš„æ˜¾ç¤º
            styled_df = styled_df.format({
                'åœºå†…æˆäº¤é¢': format_turnover
            })
            
            # æ˜¾ç¤ºè¡¨æ ¼
            st.dataframe(
                styled_df,
                width='stretch',
                height=600,
                hide_index=True
            )
            
            # å¯¼å‡ºåŠŸèƒ½
            st.markdown("---")
            csv = filtered_df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ å¯¼å‡ºç­›é€‰ç»“æœä¸º CSV",
                data=csv,
                file_name=f"LOFå¥—åˆ©æœºä¼š_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
            
        else:
            st.warning("âš ï¸ å½“å‰æ²¡æœ‰ç¬¦åˆç­›é€‰æ¡ä»¶çš„å¥—åˆ©æœºä¼š")
            st.info("ğŸ’¡ æç¤ºï¼šå°è¯•é™ä½æº¢ä»·ç‡æˆ–æˆäº¤é¢é˜ˆå€¼")
    
    with tab2:
        # æ˜¾ç¤ºå…¨é‡æ•°æ®
        st.markdown(f"**å…¨é‡æ•°æ®** - å…± {len(df)} åª LOF åŸºé‡‘")
        st.info("ğŸ’¡ æ­¤åˆ—è¡¨æ˜¾ç¤ºæ‰€æœ‰å·²è·å–å‡€å€¼çš„ LOF åŸºé‡‘ï¼ŒæŒ‰æº¢ä»·ç‡é™åºæ’åˆ—")
        st.markdown("ğŸŸ¥ **çº¢è‰²** = é«˜æº¢ä»·(â‰¥55%) | ğŸŸ¡ **é»„è‰²** = ä¸­ç­‰æº¢ä»·(2-5%)")
        
        # å¯¹å…¨é‡æ•°æ®ä¹ŸæŒ‰æº¢ä»·ç‡æ’åº
        df_sorted = df.sort_values('æº¢ä»·ç‡(%)', ascending=False)
        
        # åº”ç”¨é«˜äº®
        styled_all_df = df_sorted.style.apply(highlight_premium_level, axis=1)
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        styled_all_df = styled_all_df.format({
            'åœºå†…æˆäº¤é¢': format_turnover
        })
        
        # æ˜¾ç¤ºå…¨é‡è¡¨æ ¼
        st.dataframe(
            styled_all_df,
            width='stretch',
            height=600,
            hide_index=True
        )
        
        # å¯¼å‡ºå…¨é‡æ•°æ®
        st.markdown("---")
        csv_all = df_sorted.to_csv(index=False, encoding='utf-8-sig')
        st.download_button(
            label="ğŸ“¥ å¯¼å‡ºå…¨é‡æ•°æ®ä¸º CSV",
            data=csv_all,
            file_name=f"LOFå…¨é‡æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime="text/csv"
        )
    
    # é¡µè„š
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: gray;'>
            <p>âš ï¸ é£é™©æç¤ºï¼šå¥—åˆ©æœ‰é£é™©ï¼ŒæŠ•èµ„éœ€è°¨æ…ã€‚æœ¬ç³»ç»Ÿä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚</p>
            <p>ğŸ“Š æ•°æ®æ›´æ–°æ—¶é—´ï¼š{}</p>
        </div>
        """.format(datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main()
